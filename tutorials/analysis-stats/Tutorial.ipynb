{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial about statistical methods\n",
    "The following contains a sequence of simple exercises, designed to get familiar with using Minuit for maximum likelihood fits and emcee to determine parameters by MCMC. Commands are generally commented, i.e. in order to activate them, simply uncomment them. A few functions are still to be defined... which is part of the exercise. Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a dataset to be fitted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.random.seed(12345)\n",
    "#y = np.random.random(10000)\n",
    "#x = 1./np.sqrt(y)\n",
    "#plt.hist(x, bins=100, range=(1,10), histtype='stepfilled',color='blue')\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood fit of a simple power law\n",
    "First define the negative-log likelihood function for a density proportional to x**(-a) the range 1 < x < infinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nllp(a)\n",
    "# here define the function \n",
    "    return 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then minimize it using iminuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import iminuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# minp = iminuit.Minuit(nllp,a= ?,error_a=?, errordef=?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# minp.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis\n",
    "First determine the parabolic errors using hesse() and then do a parameter scan using minos() to determine the 68% confidence level errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# minp.hesse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# minp.minos()\n",
    "# minp.draw_profile('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of an un-normalised PDF \n",
    "The above example shall be modified such that the normalisation of the likelihood function, which so far was determined analytically, now is determined numerically in the fit. This is the more realistic case, since in many case no (simple) analytical normalisation exists. As a first step, this requires to load the integration package.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.integrate import quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pdfpn(x, a):\n",
    "    return x**(-a)\n",
    "def pdfpn_norm(a):\n",
    "# here insert the calculation of the normalisation as a function of a \n",
    "    return 1.\n",
    "def nllpn(a):\n",
    "# calculate and return the proper negative-log likelihood function\n",
    "    return 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then do the same minimization steps as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# minpn = iminuit.Minuit(nllpn, a=?, error_a=?, errordef=?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# minpn.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend the fit model by an exponential cutoff\n",
    "The exponential cutoff is implemented by exp(-b*b*x), i.e. exponential growth is not allowed for real valued parameters b. The implications of this ansatz shall be discussed when looking at the solution. After that, the example can be modified to use exp(-b*x). \n",
    "\n",
    "Here the likelihood function has no (simple) analytical normalisation anymore, i.e. we directly do the numerical approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pdfcn(x, a, b):\n",
    "    return x**(-a)*np.exp(-b*b*x)\n",
    "def pdfcn_norm(a, b):\n",
    "# determine the normalization    \n",
    "    return 1.\n",
    "def nllcn(a, b):\n",
    "# calculate an return the negative-log likelihood function\n",
    "    return 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, use Minuit for minimisation and error analysis, but now in two dimensions. Study parabolic errors and minos errors, the latter both for the single variables and for both together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mincn = iminuit.Minuit(nllcn, a=?, b=?, error_a=?, error_b=?, errordef=?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mincn.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mincn.hesse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mincn.minos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mincn.draw_profile('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mincn.draw_profile('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mincn.draw_contour('a','b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the same analysis by MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import emcee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emcee requires as input the log-likelihood of the posterior in the parameters a and b. In the following it is composed of the log-of the prior and the log-likelihood of the data. Initially use a simple uniform prior in a and b with the constraint b>0. Afterwards one can play with the prior to see how strongly it affects the result.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the posterior.\n",
    "# for clarity the prior and likelihood are separated\n",
    "# emcee requires log-posterior\n",
    "\n",
    "def log_prior(theta):\n",
    "    a, b = theta\n",
    "    if b < 0:\n",
    "        return -np.inf  # log(0)\n",
    "    else:\n",
    "        return  0.\n",
    "\n",
    "def log_likelihood(theta, x):\n",
    "    a, b = theta\n",
    "    return np.sum(-a*np.log(x) - b*b*x)\n",
    "\n",
    "def log_posterior(theta, x):\n",
    "    a , b = theta\n",
    "# construct and the log of the posterior     \n",
    "    return 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll set up the computation. emcee combines multiple \"walkers\", each of which is its own MCMC chain. The number of trace results will be nwalkers * nsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndim     = 2     # number of parameters in the model\n",
    "nwalkers = 50    # number of MCMC walkers\n",
    "nburn    = 100   # \"burn-in\" period to let chains stabilize\n",
    "nsteps   = 1000  # number of MCMC steps to take\n",
    "\n",
    "# random starting point\n",
    "np.random.seed(0)\n",
    "starting_guesses = np.random.random((nwalkers, ndim))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the MCMC (and time it using IPython's %time magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=[x])\n",
    "#%time sampler.run_mcmc(starting_guesses, nsteps)\n",
    "#print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sampler.chain is of shape (nwalkers, nsteps, ndim). Before analysis throw-out the burn-in points and reshape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#emcee_trace = sampler.chain[:, nburn:, :].reshape(-1, ndim).T\n",
    "#len(emcee_trace[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse the results. Plot the projected (marginalized) posteriors for the parameters a and b and also the joinyt density as sampled by the MCMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.hist(emcee_trace[0], 100, range=(?,?) , histtype='stepfilled', color='cyan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.hist(emcee_trace[1], 100, range=(?,?) , histtype='stepfilled', color='cyan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.plot(emcee_trace[0],emcee_trace[1],',k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, generate 2-dim bayesian confidence level contours containing 68.3% and 95.5% probability content. For that define a convenient plot functions and use them. Overlay the contours with the scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_sigma_level(trace1, trace2, nbins=20):\n",
    "    \"\"\"From a set of traces, bin by number of standard deviations\"\"\"\n",
    "    L, xbins, ybins = np.histogram2d(trace1, trace2, nbins)\n",
    "    L[L == 0] = 1E-16\n",
    "    logL = np.log(L)\n",
    "\n",
    "    shape = L.shape\n",
    "    L = L.ravel()\n",
    "\n",
    "    # obtain the indices to sort and unsort the flattened array\n",
    "    i_sort = np.argsort(L)[::-1]\n",
    "    i_unsort = np.argsort(i_sort)\n",
    "\n",
    "    L_cumsum = L[i_sort].cumsum()\n",
    "    L_cumsum /= L_cumsum[-1]\n",
    "    \n",
    "    xbins = 0.5 * (xbins[1:] + xbins[:-1])\n",
    "    ybins = 0.5 * (ybins[1:] + ybins[:-1])\n",
    "\n",
    "    return xbins, ybins, L_cumsum[i_unsort].reshape(shape)\n",
    "\n",
    "\n",
    "#xbins, ybins, sigma = compute_sigma_level(emcee_trace[0], emcee_trace[1])\n",
    "#plt.contour(xbins, ybins, sigma.T, levels=[0.683, 0.955])\n",
    "#plt.plot(emcee_trace[0], emcee_trace[1], ',k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
